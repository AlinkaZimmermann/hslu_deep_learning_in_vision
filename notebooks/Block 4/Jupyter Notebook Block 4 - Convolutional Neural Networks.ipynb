{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Convolutional Neural Networks for CIFAR-10\n",
    "\n",
    "\n",
    "In this notebook chapter, we'll build, train and optimize a neural network to classify images of the CIFAR-10 dataset using convolutional neural networks.\n",
    "\n",
    "This guide uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Train: X=(50000, 32, 32, 3), y=(50000, 1)\n",
      "Test: X=(10000, 32, 32, 3), y=(10000, 1)\n",
      "(50000, 1)\n",
      "(10000, 1)\n",
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "# Import TensorFlow and TensorFlow Datasets\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# Helper libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    " \n",
    "\n",
    "\n",
    "class_names = ['airplanes', 'cars', 'birds', 'cats', 'deer',\n",
    "               'dogs',      'frogs',   'horses',  'ships',   'trucks']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
    "print('Test: X=%s, y=%s' % (X_test.shape, y_test.shape))\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_test_cat  = to_categorical(y_test, 10)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train_cat.shape)\n",
    "print(y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess the data\n",
    "\n",
    "### 1.1 Zero-Centered Images\n",
    "\n",
    "The value of each pixel in the image data is an integer in the range `[0,255]`. For the model to work properly, these values need to be zero-centered to the range. So here we compute the mean value per colour channel over all training images and subtract the mean value per colour channel from every image in the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125.3069  122.95015 113.866  ]\n",
      "[-25.327402 -39.670845 -56.451923]\n",
      "(32, 32, 3)\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "train_mean = X_train.mean(axis=(0,1,2))\n",
    "train_std =  X_train.std(axis=(0,1,2))\n",
    "print(train_mean)\n",
    "X_train_zc = X_train - train_mean\n",
    "X_test_zc  = X_test - train_mean\n",
    "\n",
    "\n",
    "print(X_train_zc[3].mean(axis=(0,1)))\n",
    "print(X_train_zc[3].shape)\n",
    "print(X_train_zc.shape)\n",
    "print(X_test_zc.shape)\n",
    "print(X_train_zc.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False, \n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,  \n",
    "        # divide inputs by std of the dataset\n",
    "        featurewise_std_normalization=False, \n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,  \n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False, \n",
    "        # epsilon for ZCA whitening\n",
    "        zca_epsilon=1e-06,  \n",
    "        # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        rotation_range=0,  \n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        # set range for random shear\n",
    "        shear_range=0., \n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,  \n",
    "        # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        channel_shift_range=0.,  \n",
    "        # value used for fill_mode = \"constant\"\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  \n",
    "        # randomly flip images\n",
    "        horizontal_flip=True, \n",
    "        # randomly flip images\n",
    "        vertical_flip=False,  \n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "validation_datagen = ImageDataGenerator( \n",
    "      # rescale the pixel values (between 0 and 255) to the [0,1] interval \n",
    "      # rescale=1./255\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the model\n",
    "\n",
    "Building the neural network requires configuring the layers of the model, then compiling the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define the network\n",
    "\n",
    "The basic building block of a neural network is the *layer*. A layer extracts a representation from the data fed into it. Hopefully, a series of connected layers results in a representation that is meaningful for the problem at hand.\n",
    "\n",
    "Much of deep learning consists of chaining together simple layers. Most layers, like `tf.keras.layers.Dense`, have internal parameters which are adjusted (\"learned\") during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=X_train_zc.shape[1:]))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(num_classes))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras.layers.Conv2D(32, (3, 3)))` implies the following parameter settings:\n",
    "\n",
    "\n",
    "- `filters` : Integer, the dimensionality of the output space (i.e. the number of output filters in the \n",
    "convolution). In this case, the number of filters is $32$.\n",
    "\n",
    "- `kernel_size` : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window (receptive field). Can be a single integer to specify the same value for all spatial dimensions. In this \n",
    "case, the receptive field of the filters are $3\\times3$.\n",
    "\n",
    "- `strides` : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Default strides : $(1, 1)$\n",
    "\n",
    "- `padding`: one of `valid` or `same`. In this case, the size of the output volume won't be changed.\n",
    "\n",
    "`tf.keras.layers.MaxPooling2D(pool_size=(2, 2))` implies the following parameter settings:\n",
    "\n",
    "- `pool_size`: integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). $(2, 2)$ will halve the input in both spatial dimension. If only one integer is specified, the same window length will be used for both dimensions.\n",
    "- `strides`: Integer, tuple of 2 integers, or `None`. `strides` values. If `None`, it will default to `pool_size`. In this case $(2,2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 30, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 13, 13, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,253,714\n",
      "Trainable params: 1,252,286\n",
      "Non-trainable params: 1,428\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Compile and Fit the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-c83e11259bc8>:26: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 125 steps, validate for 32 steps\n",
      "Epoch 1/100\n",
      "125/125 [==============================] - 11s 87ms/step - loss: 2.2512 - accuracy: 0.2062 - val_loss: 1.9002 - val_accuracy: 0.3350\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 9s 71ms/step - loss: 1.9459 - accuracy: 0.2895 - val_loss: 1.7895 - val_accuracy: 0.3670\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 9s 69ms/step - loss: 1.8558 - accuracy: 0.3237 - val_loss: 1.7282 - val_accuracy: 0.3870\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 9s 71ms/step - loss: 1.7955 - accuracy: 0.3543 - val_loss: 1.6843 - val_accuracy: 0.4210\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 9s 69ms/step - loss: 1.7510 - accuracy: 0.3750 - val_loss: 1.6127 - val_accuracy: 0.4420\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 9s 72ms/step - loss: 1.7125 - accuracy: 0.3832 - val_loss: 1.5819 - val_accuracy: 0.4380\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 9s 70ms/step - loss: 1.6774 - accuracy: 0.4085 - val_loss: 1.5531 - val_accuracy: 0.4630\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 9s 71ms/step - loss: 1.6421 - accuracy: 0.4238 - val_loss: 1.5303 - val_accuracy: 0.4660\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 9s 71ms/step - loss: 1.6171 - accuracy: 0.4288 - val_loss: 1.4932 - val_accuracy: 0.4830\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.6094 - accuracy: 0.4268 - val_loss: 1.4944 - val_accuracy: 0.4840\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.5769 - accuracy: 0.4538 - val_loss: 1.4593 - val_accuracy: 0.5030\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 1.5610 - accuracy: 0.4620 - val_loss: 1.4482 - val_accuracy: 0.5090\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.5473 - accuracy: 0.4640 - val_loss: 1.4375 - val_accuracy: 0.5030\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 1.5314 - accuracy: 0.4683 - val_loss: 1.4114 - val_accuracy: 0.5080\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.5171 - accuracy: 0.4717 - val_loss: 1.4202 - val_accuracy: 0.5020\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.5016 - accuracy: 0.4880 - val_loss: 1.4279 - val_accuracy: 0.5110\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.4835 - accuracy: 0.4922 - val_loss: 1.3995 - val_accuracy: 0.5190\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.4871 - accuracy: 0.4945 - val_loss: 1.3945 - val_accuracy: 0.5170\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.4704 - accuracy: 0.4952 - val_loss: 1.3861 - val_accuracy: 0.5260\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 1.4580 - accuracy: 0.4983 - val_loss: 1.3804 - val_accuracy: 0.5190\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.4455 - accuracy: 0.5020 - val_loss: 1.4168 - val_accuracy: 0.5090\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.4289 - accuracy: 0.5160 - val_loss: 1.3914 - val_accuracy: 0.5210\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 7s 60ms/step - loss: 1.4294 - accuracy: 0.5155 - val_loss: 1.3497 - val_accuracy: 0.5220\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.4021 - accuracy: 0.5272 - val_loss: 1.3266 - val_accuracy: 0.5390\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.3942 - accuracy: 0.5290 - val_loss: 1.3986 - val_accuracy: 0.5090\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 8s 61ms/step - loss: 1.3884 - accuracy: 0.5257 - val_loss: 1.3250 - val_accuracy: 0.5300\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.3760 - accuracy: 0.5315 - val_loss: 1.3303 - val_accuracy: 0.5390\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.3583 - accuracy: 0.5497 - val_loss: 1.3378 - val_accuracy: 0.5320\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.3590 - accuracy: 0.5420 - val_loss: 1.3401 - val_accuracy: 0.5210\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 1.3367 - accuracy: 0.5595 - val_loss: 1.3106 - val_accuracy: 0.5380\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 1.3270 - accuracy: 0.5627 - val_loss: 1.3126 - val_accuracy: 0.5310\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.3192 - accuracy: 0.5570 - val_loss: 1.3391 - val_accuracy: 0.5420\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.3205 - accuracy: 0.5633 - val_loss: 1.2975 - val_accuracy: 0.5500\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.3031 - accuracy: 0.5730 - val_loss: 1.2648 - val_accuracy: 0.5450\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.2936 - accuracy: 0.5763 - val_loss: 1.3032 - val_accuracy: 0.5480\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.2869 - accuracy: 0.5767 - val_loss: 1.2882 - val_accuracy: 0.5460\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.2664 - accuracy: 0.5835 - val_loss: 1.2441 - val_accuracy: 0.5620\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.2705 - accuracy: 0.5910 - val_loss: 1.2517 - val_accuracy: 0.5580\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.2691 - accuracy: 0.5860 - val_loss: 1.2886 - val_accuracy: 0.5380\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 1.2612 - accuracy: 0.5925 - val_loss: 1.2918 - val_accuracy: 0.5490\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.2381 - accuracy: 0.5845 - val_loss: 1.2037 - val_accuracy: 0.5760\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.2424 - accuracy: 0.5865 - val_loss: 1.2147 - val_accuracy: 0.5640\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 1.2125 - accuracy: 0.6050 - val_loss: 1.2072 - val_accuracy: 0.5780\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 1.2021 - accuracy: 0.6102 - val_loss: 1.2310 - val_accuracy: 0.5600\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 1.2033 - accuracy: 0.6012 - val_loss: 1.2278 - val_accuracy: 0.5700\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.2009 - accuracy: 0.6040 - val_loss: 1.2098 - val_accuracy: 0.5830\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.1954 - accuracy: 0.6062 - val_loss: 1.1676 - val_accuracy: 0.5900\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.1877 - accuracy: 0.6122 - val_loss: 1.1936 - val_accuracy: 0.5750\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.1721 - accuracy: 0.6233 - val_loss: 1.2490 - val_accuracy: 0.5620\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.1670 - accuracy: 0.6190 - val_loss: 1.1896 - val_accuracy: 0.5740\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.1495 - accuracy: 0.6330 - val_loss: 1.2133 - val_accuracy: 0.5650\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 1.1429 - accuracy: 0.6245 - val_loss: 1.1587 - val_accuracy: 0.5930\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 1.1399 - accuracy: 0.6283 - val_loss: 1.2248 - val_accuracy: 0.5740\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.1417 - accuracy: 0.6298 - val_loss: 1.2083 - val_accuracy: 0.5830\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.1273 - accuracy: 0.6373 - val_loss: 1.1745 - val_accuracy: 0.5950\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 8s 64ms/step - loss: 1.1316 - accuracy: 0.6290 - val_loss: 1.1910 - val_accuracy: 0.5910\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 1.1151 - accuracy: 0.6378 - val_loss: 1.1406 - val_accuracy: 0.5990\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.1250 - accuracy: 0.6335 - val_loss: 1.1646 - val_accuracy: 0.5940\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.1107 - accuracy: 0.6338 - val_loss: 1.2260 - val_accuracy: 0.5720\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 7s 54ms/step - loss: 1.0887 - accuracy: 0.6522 - val_loss: 1.1106 - val_accuracy: 0.6010\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 9s 73ms/step - loss: 1.1023 - accuracy: 0.6420 - val_loss: 1.1708 - val_accuracy: 0.5960\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 9s 73ms/step - loss: 1.0802 - accuracy: 0.6520 - val_loss: 1.1983 - val_accuracy: 0.5830\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 9s 71ms/step - loss: 1.0874 - accuracy: 0.6457 - val_loss: 1.1665 - val_accuracy: 0.5900\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 9s 76ms/step - loss: 1.0830 - accuracy: 0.6495 - val_loss: 1.1267 - val_accuracy: 0.6140\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 9s 74ms/step - loss: 1.0817 - accuracy: 0.6430 - val_loss: 1.1445 - val_accuracy: 0.6040\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 9s 75ms/step - loss: 1.0618 - accuracy: 0.6570 - val_loss: 1.0997 - val_accuracy: 0.6280\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 9s 75ms/step - loss: 1.0612 - accuracy: 0.6557 - val_loss: 1.1619 - val_accuracy: 0.5990\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 9s 70ms/step - loss: 1.0453 - accuracy: 0.6640 - val_loss: 1.1093 - val_accuracy: 0.6200\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 1.0426 - accuracy: 0.6633 - val_loss: 1.1304 - val_accuracy: 0.6060\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.0435 - accuracy: 0.6690 - val_loss: 1.1341 - val_accuracy: 0.6130\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 7s 60ms/step - loss: 1.0272 - accuracy: 0.6662 - val_loss: 1.0605 - val_accuracy: 0.6390\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 1.0287 - accuracy: 0.6668 - val_loss: 1.1215 - val_accuracy: 0.6040\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 1.0202 - accuracy: 0.6737 - val_loss: 1.0863 - val_accuracy: 0.6280\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.0168 - accuracy: 0.6768 - val_loss: 1.0998 - val_accuracy: 0.6190\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 1.0147 - accuracy: 0.6755 - val_loss: 1.1142 - val_accuracy: 0.6180\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 0.9988 - accuracy: 0.6860 - val_loss: 1.1207 - val_accuracy: 0.6120\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 1.0097 - accuracy: 0.6752 - val_loss: 1.1374 - val_accuracy: 0.6100\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.9933 - accuracy: 0.6870 - val_loss: 1.0798 - val_accuracy: 0.6240\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.9892 - accuracy: 0.6795 - val_loss: 1.1673 - val_accuracy: 0.6030\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 0.9884 - accuracy: 0.6880 - val_loss: 1.1543 - val_accuracy: 0.5990\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.9772 - accuracy: 0.6855 - val_loss: 1.0510 - val_accuracy: 0.6410\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.9591 - accuracy: 0.6950 - val_loss: 1.0653 - val_accuracy: 0.6260\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.9610 - accuracy: 0.6973 - val_loss: 1.1048 - val_accuracy: 0.6180\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 0.9459 - accuracy: 0.6998 - val_loss: 1.1615 - val_accuracy: 0.6030\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 0.9553 - accuracy: 0.6973 - val_loss: 1.1108 - val_accuracy: 0.6230\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 0.9471 - accuracy: 0.6977 - val_loss: 1.0415 - val_accuracy: 0.6480\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.9556 - accuracy: 0.7000 - val_loss: 1.0634 - val_accuracy: 0.6290\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 0.9448 - accuracy: 0.6995 - val_loss: 1.0514 - val_accuracy: 0.6410\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.9378 - accuracy: 0.6973 - val_loss: 1.0780 - val_accuracy: 0.6270\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 0.9305 - accuracy: 0.7030 - val_loss: 1.0705 - val_accuracy: 0.6250\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.9114 - accuracy: 0.7225 - val_loss: 1.0238 - val_accuracy: 0.6440\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 0.9195 - accuracy: 0.7135 - val_loss: 1.0617 - val_accuracy: 0.6390\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 0.9175 - accuracy: 0.7100 - val_loss: 1.0606 - val_accuracy: 0.6510\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 0.9050 - accuracy: 0.7190 - val_loss: 1.0665 - val_accuracy: 0.6400\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 7s 56ms/step - loss: 0.9041 - accuracy: 0.7163 - val_loss: 1.0948 - val_accuracy: 0.6290\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 0.8954 - accuracy: 0.7147 - val_loss: 1.0434 - val_accuracy: 0.6530\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 0.9040 - accuracy: 0.7168 - val_loss: 1.0383 - val_accuracy: 0.6420\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 0.8882 - accuracy: 0.7168 - val_loss: 1.0157 - val_accuracy: 0.6480\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.9034 - accuracy: 0.7132 - val_loss: 0.9803 - val_accuracy: 0.6670\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.8893 - accuracy: 0.7197 - val_loss: 1.0824 - val_accuracy: 0.6320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Model.evaluate of <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fee2f13a490>>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime, os\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "# Compile Network \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, decay=1e-6),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "num_validation_examples = 1000\n",
    "num_train_examples = 5000\n",
    "\n",
    "\n",
    "train_iterator = train_datagen.flow(X_train_zc[num_validation_examples:num_train_examples], \n",
    "                                    y_train_cat[num_validation_examples:num_train_examples], \n",
    "                                    batch_size=32)\n",
    "validation_iterator = validation_datagen.flow(X_train_zc[:num_validation_examples:], \n",
    "                                              y_train_cat[:num_validation_examples:], \n",
    "                                              batch_size=32)\n",
    "\n",
    "# Fit Network\n",
    "history = model.fit_generator(generator= train_iterator,  \n",
    "                              validation_data = validation_iterator, \n",
    "                              epochs=100, \n",
    "                              steps_per_epoch=len(train_iterator),\n",
    "                              callbacks=[tensorboard_callback])\n",
    "\n",
    "# Determine \n",
    "model.evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluate the Network on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 432us/sample - loss: 1.0911 - accuracy: 0.6336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.091132074546814, 0.6336]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_zc, y_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation accuracy\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(100)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9ebfdb0f64ab972b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9ebfdb0f64ab972b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.makedirs(logdir, exist_ok=True)\n",
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = './Daten/cifar_10_convnets_1.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-85fb2bf19c5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Daten/cifar_10_convnets_1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \"\"\"\n\u001b[1;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1008\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    110\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    111\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 112\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    425\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    426\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n\u001b[0;32m--> 427\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = './Daten/cifar_10_convnets_1.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "model.save('./Daten/cifar_10_convnets_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FE7KNzPPVrVV"
   },
   "source": [
    "# Part 2 - Dogs vs Cats Image Classification : Visualizing what ConvNets learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gN7G9GFmVrVY"
   },
   "source": [
    "In this notebook chapter, we will discuss how to classify images into pictures of cats or pictures of dogs. We'll build an image classifier using `tf.keras.Sequential` model and load data using `tf.keras.preprocessing.image.ImageDataGenerator`. At the end, we will visualize filters and intermediate activation layers.\n",
    "\n",
    "## Specific concepts that will be covered:\n",
    "In the process, we will build practical experience and develop intuition around the following concepts\n",
    "\n",
    "* Building _data input pipelines_ using the `tf.keras.preprocessing.image.ImageDataGenerator` class  How can we efficiently work with data on disk to interface with our model?\n",
    "* _Overfitting_ - what is it, how to identify it, and how can we prevent it?\n",
    "* _Data Augmentation_ and _Dropout_ - Key techniques to fight overfitting in computer vision tasks that we will incorporate into our data pipeline and image classifier model.\n",
    "* _Visualizing ConvNet filters_\n",
    "* _Visualizing intermediate activations_\n",
    "\n",
    "\n",
    "## We will follow the general machine learning workflow:\n",
    "\n",
    "1. Examine and understand data\n",
    "2. Build an input pipeline\n",
    "3. Build our model\n",
    "4. Train our model\n",
    "5. Test our model\n",
    "6. Improve our model/Repeat the process\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zF9uvbXNVrVY"
   },
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VddxeYBEVrVZ"
   },
   "source": [
    "Let's start by importing required packages:\n",
    "\n",
    "*   os  to read files and directory structure\n",
    "*   numpy  for some matrix math outside of TensorFlow\n",
    "*   matplotlib.pyplot  to plot the graph and display images in our training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rtPGh2MAVrVa"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1WtoaOHVrVh"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZZI6lNkVrVm"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPHx8-t-VrVo"
   },
   "source": [
    "To build our image classifier, we begin by downloading the dataset. The dataset we are using is a filtered version of <a href=\"https://www.kaggle.com/c/dogs-vs-cats/data\" target=\"_blank\">Dogs vs. Cats</a> dataset from Kaggle (ultimately, this dataset is provided by Microsoft Research).\n",
    "\n",
    "In this Jupyter Notebook however, we will make use of the class `tf.keras.preprocessing.image.ImageDataGenerator` which will read data from disk. We therefore need to directly download *Dogs vs. Cats* from a URL and unzip it to your computers filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYmOylPlVrVt"
   },
   "outputs": [],
   "source": [
    "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "\n",
    "zip_dir = tf.keras.utils.get_file('cats_and_dogs_filterted.zip', origin=_URL, extract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Giv0wMQzVrVw"
   },
   "source": [
    "The dataset we have downloaded has following directory structure.\n",
    "\n",
    "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
    "<b>cats_and_dogs_filtered</b>\n",
    "|__ <b>train</b>\n",
    "    |______ <b>cats</b>: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....]\n",
    "    |______ <b>dogs</b>: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]\n",
    "|__ <b>validation</b>\n",
    "    |______ <b>cats</b>: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....]\n",
    "    |______ <b>dogs</b>: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpmywIlsVrVx"
   },
   "source": [
    "We'll now assign variables with the proper file path for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRucI3QqVrVy"
   },
   "outputs": [],
   "source": [
    "print(os.path.dirname(zip_dir))\n",
    "base_dir = os.path.join(os.path.dirname(zip_dir), 'cats_and_dogs_filtered')\n",
    "print(base_dir)\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "print(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "print(validation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Utv3nryxVrV0"
   },
   "outputs": [],
   "source": [
    "train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\n",
    "print(train_cats_dir)\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\n",
    "print(train_dogs_dir)\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\n",
    "print(validation_cats_dir)\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures\n",
    "print(validation_dogs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZdrHHTy2VrV3"
   },
   "source": [
    "### Understanding our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LblUYjl-VrV3"
   },
   "source": [
    "Let's look at how many cats and dogs images we have in our training and validation directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vc4u8e9hVrV4"
   },
   "outputs": [],
   "source": [
    "num_cats_tr = len(os.listdir(train_cats_dir))\n",
    "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
    "\n",
    "num_cats_val = len(os.listdir(validation_cats_dir))\n",
    "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
    "\n",
    "total_train = num_cats_tr + num_dogs_tr\n",
    "total_val = num_cats_val + num_dogs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4GGzGt0VrV7"
   },
   "outputs": [],
   "source": [
    "print('total training cat images:', num_cats_tr)\n",
    "print('total training dog images:', num_dogs_tr)\n",
    "\n",
    "print('total validation cat images:', num_cats_val)\n",
    "print('total validation dog images:', num_dogs_val)\n",
    "print(\"--\")\n",
    "print(\"Total training images:\", total_train)\n",
    "print(\"Total validation images:\", total_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdsI_L-NVrV_"
   },
   "source": [
    "# Setting Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Lp-0ejxOtP1"
   },
   "source": [
    "For convenience, let us set up variables that will be used later while pre-processing our dataset and training our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NqNselLVrWA"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "IMG_SHAPE  = 150 # Our training data consists of images with width of 150 pixels and height of 150 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RLciCR_FVrWH"
   },
   "source": [
    "After defining our generators for training and validation images, **flow_from_directory** method will load images from the disk and will apply rescaling and will resize them into required dimensions using single line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOoVpxFwVrWy"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wn_QLciWVrWy"
   },
   "source": [
    "Overfitting often occurs when we have a small number of training examples. One way to fix this problem is to augment our dataset so that it has sufficient number and variety of training examples. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples through random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This exposes the model to more aspects of the data, allowing it to generalize better.\n",
    "\n",
    "In **tf.keras** we can implement this using the same **ImageDataGenerator** class we used in the last teaching unit. We can simply pass different transformations we want to our dataset as a form of arguments and it will take care of applying it to the dataset during our training process.\n",
    "\n",
    "To start off, let's define a function that can display an image, so we can see the type of augmentation that has been performed. Then, we'll look at specific augmentations that we'll use during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBYLOFgOXPJ9"
   },
   "outputs": [],
   "source": [
    "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rlVj6VqaVrW0"
   },
   "source": [
    "### Flipping the image horizontally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xcdvx4TVVrW1"
   },
   "source": [
    "We can begin by randomly applying horizontal flip augmentation to our dataset and seeing how individual images will look after the transformation. This is achieved by passing `horizontal_flip=True` as an argument to the `ImageDataGenerator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bi1_vHyBVrW2"
   },
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
    "\n",
    "train_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                                               directory=train_dir,\n",
    "                                               shuffle=True,\n",
    "                                               target_size=(IMG_SHAPE,IMG_SHAPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJpRSxJ-VrW7"
   },
   "source": [
    "from `IPython.display` import `display`, `ImageTo` see the transformation in action, let's take one sample image from our training set and repeat it five times. The augmentation will be randomly applied (or not) to each repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrKGd_jjVrW7"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# if not installed, execute !pip3 install pillow\n",
    "# then restart kernel\n",
    "import PIL.Image\n",
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7n9xcqCVrXB"
   },
   "source": [
    "### Rotating the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXnwkzFuVrXB"
   },
   "source": [
    "The rotation augmentation will randomly rotate the image up to a specified number of degrees. Here, we'll set it to 45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zip35pDVrXB"
   },
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rescale=1./255, rotation_range=45)\n",
    "\n",
    "train_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                                               directory=train_dir,\n",
    "                                               shuffle=True,\n",
    "                                               target_size=(IMG_SHAPE, IMG_SHAPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "deaqZLsfcZ15"
   },
   "source": [
    "To see the transformation in action, let's once again take a sample image from our training set and repeat it. The augmentation will be randomly applied (or not) to each repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVoWh4OIVrXD"
   },
   "outputs": [],
   "source": [
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FOqGPL76VrXM"
   },
   "source": [
    "### Applying Zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvqXaD8BVrXN"
   },
   "source": [
    "We can also apply Zoom augmentation to our dataset, zooming images up to 50% randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGNKLa_YVrXR"
   },
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.5)\n",
    "\n",
    "train_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                                               directory=train_dir,\n",
    "                                               shuffle=True,\n",
    "                                               target_size=(IMG_SHAPE, IMG_SHAPE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WgPWieSZcctO"
   },
   "source": [
    "One more time, take a sample image from our training set and repeat it. The augmentation will be randomly applied (or not) to each repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOvTs32FVrXU"
   },
   "outputs": [],
   "source": [
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "usS13KCNVrXd"
   },
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OC8fIsalVrXd"
   },
   "source": [
    "We can apply all these augmentations, and even others, with just one line of code, by passing the augmentations as arguments with proper values.\n",
    "\n",
    "Here, we have applied rescale, rotation of 45 degrees, width shift, height shift, horizontal flip, and zoom augmentation to our training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnr2xujaVrXe"
   },
   "outputs": [],
   "source": [
    "image_gen_train = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "train_data_gen = image_gen_train.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                                                     directory=train_dir,\n",
    "                                                     shuffle=True,\n",
    "                                                     target_size=(IMG_SHAPE,IMG_SHAPE),\n",
    "                                                     class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AW-pV5awVrXl"
   },
   "source": [
    "Let's visualize how a single image would look like five different times, when we pass these augmentations randomly to our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2m68eMhVrXm"
   },
   "outputs": [],
   "source": [
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J8cUd7FXVrXq"
   },
   "source": [
    "### Creating Validation Data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a99fDBt7VrXr"
   },
   "source": [
    "Generally, we only apply data augmentation to our training examples, since the original images should be representative of what our model needs to manage. So, in this case we are only rescaling our validation images and converting them into batches using ImageDataGenerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54x0aNbKVrXr"
   },
   "outputs": [],
   "source": [
    "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_data_gen = image_gen_val.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                                                 directory=validation_dir,\n",
    "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
    "                                                 class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5Ej-HLGVrWZ"
   },
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wEgW4i18VrWZ"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "The model consists of four convolution blocks with a max pool layer in each of them.\n",
    "\n",
    "Before the final Dense layers, we're also applying a Dropout probability of 0.5. It means that 50% of the values coming into the Dropout layer will be set to zero. This helps to prevent overfitting.\n",
    "\n",
    "Then we have a fully connected layer with 512 units, with a `relu` activation function. The model will output class probabilities for two classes  dogs and cats  using `softmax`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "Evjf8jZk2zi-"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DADWLqMSJcH3"
   },
   "source": [
    "### Compiling the model\n",
    "\n",
    "As usual, we will use the `adam` optimizer. Since we output a softmax categorization, we'll use `sparse_categorical_crossentropy` as the loss function. We would also like to look at training and validation accuracy on each epoch as we train our network, so we are passing in the metrics argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08rRJ0sn3Tb1"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uurnCp_H4Hj9"
   },
   "source": [
    "### Model Summary\n",
    "\n",
    "Let's look at all the layers of our network using **summary** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b66qAJF_4Jnw"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N06iqE8VVrWj"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oub9RtoFVrWk"
   },
   "source": [
    "It's time we train our network.\n",
    "\n",
    "Since our batches are coming from a generator (`ImageDataGenerator`), we'll use `fit_generator` instead of `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tk5NT1PW3j_P"
   },
   "outputs": [],
   "source": [
    "epochs=100\n",
    "history = model.fit_generator(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))),\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=int(np.ceil(total_val / float(BATCH_SIZE)))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojJNteAGVrWo"
   },
   "source": [
    "### Visualizing results of the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZPYT-EmVrWo"
   },
   "source": [
    "We'll now visualize the results we get after training our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8CfngybnFHQR"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./Daten/cats_and_dogs_small_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "In this Notebook Chapter, we'll take a trained model, load it into to Keras, and try it out.\n",
    "\n",
    "The model that we'll use is VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the loss tensor for filter visualization\n",
    "\n",
    "model_vgg16 = tf.keras.applications.VGG16(weights='imagenet',\n",
    "                                   include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it on a single image\n",
    "\n",
    "Remember our `model_vgg16` object is still the full VGG16 model trained on ImageNet, so it has 1000 possible output classes.\n",
    "ImageNet has a lot of dogs and cats in it, so let's see if it can predict the images in our Dogs vs. Cats dataset.\n",
    "\n",
    "Next, we we'll get an input image -- a picture of a cat, not part of the images \n",
    "the network was trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "\n",
    "img_path = './Daten/cat_1700.jpg'\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "img_tensor_orig = image.img_to_array(img)\n",
    "img_tensor_orig = np.expand_dims(img_tensor_orig, axis=0)\n",
    "img_tensor =img_tensor_orig/255\n",
    "\n",
    "print(img_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the test picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_vgg16.predict(img_tensor_orig)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a 1001 element vector of logits, rating the probability of each class for the image.\n",
    "\n",
    "So the top class ID can be found with argmax. But how can we know what class this actually is and in particular if that class ID in the ImageNet dataset denotes a cat or something else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = np.argmax(result[0], axis=-1)\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode the predictions\n",
    "\n",
    "To see what our `predicted_class` is in the ImageNet dataset, download the ImageNet labels and fetch the row that the model predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.keras.applications.vgg16.decode_predictions(result, top=3))\n",
    "\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.axis('off')\n",
    "predicted_class_name = tf.keras.applications.vgg16.decode_predictions(result, top=1)[0][0][1]\n",
    "_ = plt.title(\"Prediction: \" + predicted_class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not correct. Let us see, whether we can do it better with our model we have trained \n",
    "before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./Daten/cats_and_dogs_small_1.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to resize the photo of the cat to the size $150\\times 150$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './Daten/cat_1700.jpg'\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "img_tensor /=255\n",
    "\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(img_tensor)\n",
    "print(result)\n",
    "predicted_class = np.argmax(result[0], axis=-1)\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subdirectory cat comes before dog, therefore the class labels are assigned the integers: _cat=0, dog=1_. Thus, the prediction is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing what convnets learn\n",
    "\n",
    "\n",
    "It is often said that deep-learning models are \"black-boxes\" : learning representations that are difficult to extract and present in a human-readable form. Although this is partially true \n",
    "for certain types of deep-learning models, it is definitely not true for ConvNets. The representations learned by ConvNets are highly amenable to visualization, in large part because they are _representations of visual concepts_. Since 2013, a wide array of techniques have been developed for visualizing and interpreting these representations. We won't survey all of them, but we will cover three of the most accessible and useful ones:\n",
    "\n",
    "- _Visualizing intermediate ConvNet outputs (intermediate activations)_ -- Useful for understanding how successive convnet layers transform their input, and for getting a first idea of the meaning of individual ConvNet filters.\n",
    "\n",
    "- _Visualizing ConvNet filters_ -- Useful for understanding precisely what visual pattern or concept each filter in a convnet is receptive to. \n",
    "\n",
    "- _Visualizing heatmaps of class activation in an image_ -- Useful for understanding which parts of an image were identified as belonging to a given class, thus allowing you to localize objects in images.\n",
    "\n",
    "For the first method - activation visualization - we will use the small ConvNet that we have trained from scratch on the dogs-versus-cats classification problem before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualizing intermediate layers\n",
    "\n",
    "Visualizing intermediate layers consists of displaying the activation maps that are output by various convolution and pooling layers in a network, given a certain input (the output of a layer is often called its __activation maps__, the output of the activation function). \n",
    "\n",
    "This gives a view into how an input is decomposed into the different filters learned by the network. You want to \n",
    "visualize activation maps with three dimensions: width, height, and depth (number of filters = number of activation maps). Eeach activation map encodes relatively independent features, so the proper way to visualize these activation maps is by independently plotting the contents of every activation map as a 2D image. Lat's start by loading the model that we saved before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./Daten/cats_and_dogs_small_1.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we we'll get an input image -- a picture of a cat, not part of the images \n",
    "the network was trained on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './Daten/cat_1700.jpg'\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "img_tensor /=255\n",
    "\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the test picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract the activation maps we want to look at, we will create a Keras model \n",
    "that takes batches of images as input, and outputs the activation maps of all convolutional \n",
    "and pooling layers. To do this, we will use the Keras class `Model`. \n",
    "\n",
    "A model is instantiated using two arguments: an input tensor (or list of input tensors) \n",
    "and an output tensor (or list of output tensors). The resulting class is a Keras model, just \n",
    "like the `Sequential` models we are familiar with, mapping the specified inputs to the specified outputs. \n",
    "\n",
    "What sets the `Model` class apart is that it allows for models with multiple outputs, unlike `Sequential`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fed an image input, this model returns the values of the activation maps in the original \n",
    "model. Now we will encounter a multi-output model : until now, \n",
    "the models you have seen have had exactly one input and one output. In the general case, a model \n",
    "can have any number of inputs and outputs. This one has one input and eight outputs : one output per intermediate layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the model in predict mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_activation_maps = activation_model.predict(img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, this is the output volume of the first convolution layer for the cat image input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_activation_maps = layer_activation_maps[0]\n",
    "print(first_layer_activation_maps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a 148x148x32 output volume with 32 activation maps. Let's try plotting the first activation map \n",
    "of the activation volume of the original model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(first_layer_activation_maps[0, :, :, 0], cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activation map appears to encode a diagonal edge detector. Let's try the seventh activation map of the first layer - \n",
    "but note that your own activation maps may vary, because the specific filters learned by convolutional layers \n",
    "aren't deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the seventh channel\n",
    "plt.matshow(first_layer_activation_maps[0, :, :, 6], cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one looks like a bright white dot detector, useful to encode cat eyes. At this \n",
    "point, let's plot a complete visualization of all the activations in the network. \n",
    "You will extract and plot every channel in each of the eight activation maps, and you will \n",
    "stack the results in one big image tensor, with channels stacked side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing every channel in every intermediate activation\n",
    "\n",
    "# Names of the layers, so we can \n",
    "# have them as part of our plot\n",
    "layer_names = []\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "    \n",
    "\n",
    "images_per_row = 16\n",
    "\n",
    "# displays the feature maps\n",
    "for layer_name, layer_activation in zip(layer_names, layer_activation_maps):\n",
    "    # number of features in the feature map\n",
    "    n_features = layer_activation.shape[-1]\n",
    "    # the feature map has shape (I, size, size, n_features)\n",
    "    size = layer_activation.shape[1]\n",
    "    # tiles the activation channels in this matrix\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "    # tiles each filter into a big horizontal grid\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            for row in range(images_per_row):\n",
    "                channel_image = layer_activation[0, :, :, col * images_per_row + row]\n",
    "                \n",
    "                # post-processes the feature to make it visually palatable\n",
    "                \n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "                display_grid[col * size : (col + 1) * size,\n",
    "                             row * size : (row + 1) * size] = channel_image\n",
    "                    \n",
    "                    \n",
    "        scale = 1. / size\n",
    "        plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                            scale * display_grid.shape[0]))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "                 \n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to note here:\n",
    "\n",
    "- The first layer acts as a collection of various edge detectors. At that stage, the activations retain almost all of the information present in the initial picture.\n",
    "\n",
    "- As you go higher, the activations become increasingly abstract and less visually interpretable. They begin to encode higher-level concepts such as \"cat ear\" and \"cat eye\". Higher presentations carry increasingly less information about the visual contents of the image, and increasingly more information related to the class of the image.\n",
    "\n",
    "- The sparsity of the activations increases with the depth of the layer : in the first layer, all filters are activated by the input image; but in the following layers, more and more filters are blank. This means the pattern encoded by the filter is not found in the input image. \n",
    "\n",
    "\n",
    "We have just evidenced an important universal characteristics  of the representations learned by deep neural networks: the features extracted by a layer become increasingly abstract with the depth of the layer. The activations of higher layers carry less and less information about the specific input being seen, and more and more information about the target (in this case, the class fo the image: cat or dog). A deep neural network effectively acts as an _information distillation pipeline_, with raw data going in (in this case, RGB pictures) and being repeatedly transformed so that irrelevant information is magnified and refined (for example, the specific visual appearance of the image), and useful information is magnified and refined (for example, the class of the image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing Convnet Filters\n",
    "\n",
    "We can access all of the layers of the model via the `model.layers` property.\n",
    "\n",
    "Each layer has a `layer.name` property, where the convolutional layers have a naming convolution like `block#_conv#`, where the `#` is an integer. Therefore, we can check the name of each layer and skip any that dont contain the string `conv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize filter shapes\n",
    "for layer in model.layers:\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each convolutional layer has two sets of weights. One is the block of filters and the other is the block of bias values. These are accessible via the `layer.get_weights()` function. We can retrieve these weights and then summarize their shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filter weights\n",
    "filters, biases = layer.get_weights()\n",
    "print(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tying this together, the complete example of summarizing the model filters is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize filters in each convolutional layer\n",
    "import matplotlib.pyplot as plt\n",
    "# load the model\n",
    "\n",
    "# summarize filter shapes\n",
    "for layer in model.layers:\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    # get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all convolutional layers use 33 filters, which are small and perhaps easy to interpret.\n",
    "\n",
    "We can retrieve the filters from the first layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve weights from the first hidden layer\n",
    "filters, biases = model.layers[0].get_weights()\n",
    "print(filters.shape, biases.shape, filters.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all 32 filters\n",
    "n_filters = 32\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    ax = plt.subplot(8, 4 , i+1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.imshow(f)\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the input image with three channels for red, green and blue, that each filter has a depth of three (here we are working with a channel-last format). We could visualize one filter as a plot with three images, one for each channel, or compress all three down to a single color image, or even just look at the first channel and assume the other channels will look the same. The problem is, we then have 63 other filters that we might like to visualize.\n",
    "\n",
    "\n",
    "Now we can enumerate the first six filters out of the 32 in the block and plot each of the three channels of each filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first few filters\n",
    "n_filters, ix = 6, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(f[:, :, j], cmap='gray')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Cats vs Dogs with Data Augmentation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
