{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I : Deep Dreams\n",
    "\n",
    "Unzip the `Bilder.zip` file in the same directory where you run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_DeepDream_ is an artistic image-modification technique that uses the representations learned by convolutional neural networks. It was first released by Google in the summer of 2015, as an implementation written using the Caffee deep-learning library (this was several months before the first public release of TensorFlow). It quickly became an internet sensation thanks to the trippy pictures it could generate (see, for example, the Figure below), full of algorithmic pareidolia artifacts, bird feathers, and dog eyes - a byproduct of the fact that the Deep Dream ConvNet was trained on  ImageNet, where dog breeds and bird species are vastly overrepresented. \n",
    "\n",
    "<img src='./Bilder/Aurelia-aurita-3-0009.jpg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepDream algorithm is almost identical to ConvNet filter-visualization techniques consisting of running a ConvNet in reverse: doing gradient ascent on the input to the ConvNet in order to maximize the activation of a specific filter in an upper layer of the ConvNet. DeepDream uses this same idea, with a few simple differences:\n",
    "\n",
    "- With DeepDream, we try to maximize the activation of entire layers rather than that of a specific filter, thus mixing together visualizations of large numbers of features at once.\n",
    "\n",
    "- You start not from blank, slightly noisy input, but rather from an existing image -- thus the resulting effects latch on to preexisting visual patterns, distorting elements of the image in a somewhat artistic fashion.\n",
    "\n",
    "- The input images are processed at different scales (called _octaves_), which improves the quality of the visualizations.\n",
    "\n",
    "Let's make some DeepDreams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing DeepDream in Keras\n",
    "\n",
    "We will start from a ConvNet pretrained on ImageNet. In Keras, many such ConvNets are available: VGG16, VGG19, Xception, ResNet50, and so on. You can implement DeepDream with any of them, but your ConvNet of choice will naturally affect your visualizations, because different ConvNet architectures result in different learned features. \n",
    "The ConvNet used in the original DeepDream release was an Inception model, and in practice Inception is known to produce nice-looking DeepDreams, so we will use the Inception V3 model that comes with Keras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the pretrained Inception V3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import inception_v3\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Builds the Inception V3 network without its convolutional base.\n",
    "# The model will be loaded with pretrained ImageNet weights:\n",
    "model = inception_v3.InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks to __Keras Backend__ : Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle low-level operations such as tensor products, convolutions and so on itself. Instead, it relies on a specialized, well optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. Rather than picking one single tensor library and making the implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras. At this time, Keras has three backend implementations available: the TensorFlow backend, the Theano backend, and the CNTK backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function for DeepDreams\n",
    "\n",
    "Next, we will compute the __loss__: the quantity we will seek to maximize during the gradient-ascent process. For filter visualization, we try to maximize the value of a specific filter in a specific layer. Here, we will simultaneously maximize the activation of all filters in a number of layers. Specifically, we will maximize a weighted sum of the L2 norm of the activations of a set of high-level layers.\n",
    "\n",
    "The exact set of layers we choose (as well as their contribution to the final loss) has a major influence on the visuals we will be able to produce, so we want to make these parameters easily configurable. Lower layers result in geometric patterns, whereas higher layers result in visuals in which we can recognize some classes from ImagNet (for \n",
    "example, birds or dogs). We will start from a somewhat arbitrary configuration involving four layers - but we will definitely want to explore many different configurations later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the DeepDream Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the names of the layers\n",
    "# for which we try to maximize activation,\n",
    "# as well as their weight in the final loss\n",
    "# we try to maximize.\n",
    "# You can tweak these setting to obtain new visual effects.\n",
    "layer_settings = {\n",
    "    \"mixed4\": 1.0,\n",
    "    \"mixed5\": 1.5,\n",
    "    \"mixed6\": 2.0,\n",
    "    \"mixed7\": 2.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a tensor that contains the __loss__ : the weighted sum of the L2 norm of the activations of the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the loss to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict(\n",
    "    [\n",
    "        (layer.name, layer.output)\n",
    "        for layer in [model.get_layer(name) for name in layer_settings.keys()]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set up a model that returns the activation values for every target layer\n",
    "# (as a dict)\n",
    "feature_extractor = tf.keras.Model(inputs=model.inputs, outputs=outputs_dict)\n",
    "\n",
    "def compute_loss(input_image):\n",
    "    features = feature_extractor(input_image)\n",
    "    # Initialize the loss\n",
    "    loss = tf.zeros(shape=())\n",
    "    for name in features.keys():\n",
    "        coeff = layer_settings[name]\n",
    "        activation = features[name]\n",
    "        # We avoid border artifacts by only involving non-border pixels in the loss.\n",
    "        scaling = tf.reduce_prod(tf.cast(tf.shape(activation), \"float32\"))\n",
    "        loss += coeff * tf.reduce_sum(tf.square(activation[:, 2:-2, 2:-2, :])) / scaling\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can set up the gradient-ascent process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient-ascent process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent_step(img, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img)\n",
    "        loss = compute_loss(img)\n",
    "    # Compute gradients.\n",
    "    grads = tape.gradient(loss, img)\n",
    "    # Normalize gradients.\n",
    "    grads /= tf.maximum(tf.reduce_mean(tf.abs(grads)), 1e-6)\n",
    "    img += learning_rate * grads\n",
    "    return loss, img\n",
    "\n",
    "\n",
    "def gradient_ascent_loop(img, iterations, learning_rate, max_loss=None):\n",
    "    for i in range(iterations):\n",
    "        loss, img = gradient_ascent_step(img, learning_rate)\n",
    "        if max_loss is not None and loss > max_loss:\n",
    "            break\n",
    "        print(\"... Loss value at step %d: %.2f\" % (i, loss))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the actual DeepDreams algorithm. First, we define a list of _scales_ (also called _octaves_) \n",
    "at which to process the images. Each successive scale is larger than the previous one by a factor of 1.4 (it's 40% larger): you start by processing a small image and then increasingly scale it up. \n",
    "\n",
    "\n",
    "<img src='./Bilder/octaves.jpg'>\n",
    "\n",
    "\n",
    "For each successive scale, from the smallest to the largest, you run gradient ascent to maximize the loss you previously defined, at that scale. After each gradient ascent run, you upscale the resulting image by $40\\%$. \n",
    "\n",
    "To avoid losing a lot of image detail after each successive scale-up (resulting in increasingly blurry or pixelated images), you can use a simple trick: after each scale-up, you will reinject the lost details back into the image, which is possible because you know what the original image should look like at the larger scale. Given a small image\n",
    "size $S$ and a larger image size $L$, you can compute the difference between the original\n",
    "image resized to size $L$ and the original resized to size $S$ — this difference quantifies the\n",
    "details lost when going from $S$ to $L$.\n",
    "\n",
    "Let dreams come up over lake Sils in Engadin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this with the path to the image you want to use.\n",
    "base_image_path = './Bilder/sils.jpg'\n",
    "display(Image(base_image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    # Util function to open, resize and format pictures\n",
    "    # into appropriate arrays.\n",
    "    img = keras.preprocessing.image.load_img(image_path)\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Util function to convert a NumPy array into a valid image.\n",
    "    x = x.reshape((x.shape[1], x.shape[2], 3))\n",
    "    # Undo inception v3 preprocessing\n",
    "    x /= 2.0\n",
    "    x += 0.5\n",
    "    x *= 255.0\n",
    "    # Convert to uint8 and clip to the valid range [0, 255]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running gradient ascent over different successive scales\n",
    "\n",
    "#### Process:\n",
    "\n",
    "1. Load the original image.\n",
    "2. Define a number of processing scales (i.e. image shapes),\n",
    "    from smallest to largest.\n",
    "3. Resize the original image to the smallest scale.\n",
    "4. For every scale, starting with the smallest (i.e. current one):\n",
    "5. Run gradient ascent\n",
    "6. Upscale image to the next scale\n",
    "7. Reinject the detail that was lost at upscaling time\n",
    "8. Stop when we are back to the original size.\n",
    "\n",
    "To obtain the detail lost during upscaling, we simply\n",
    "take the original image, shrink it down, upscale it,\n",
    "and compare the result to the (resized) original image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill this with the path to the image you want to use.\n",
    "base_image_path = './Bilder/sils.jpg'\n",
    "\n",
    "\n",
    "# Playing with these hyperparameters\n",
    "# will let you achieve new effects.\n",
    "\n",
    "step = 0.01 # Gradient ascent step size\n",
    "num_octave = 3 # Number of scales at which to run\n",
    "               # gradient ascent\n",
    "octave_scale = 1.4 # Size ratio between scales\n",
    "iterations = 20 # Number of ascent steps to\n",
    "                # run at each scale\n",
    "\n",
    "# If the loss grows larger than 15, you’ll interrupt\n",
    "# the gradient-ascent process to avoid ugly artifacts.    \n",
    "max_loss = 15. \n",
    "\n",
    "original_img = preprocess_image(base_image_path)\n",
    "result_prefix = \"sils_maria_dream\"\n",
    "original_shape = original_img.shape[1:3]\n",
    "\n",
    "successive_shapes = [original_shape]\n",
    "for i in range(1, num_octave):\n",
    "    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
    "    successive_shapes.append(shape)\n",
    "successive_shapes = successive_shapes[::-1]\n",
    "shrunk_original_img = tf.image.resize(original_img, successive_shapes[0])\n",
    "\n",
    "img = tf.identity(original_img)  # Make a copy\n",
    "for i, shape in enumerate(successive_shapes):\n",
    "    print(\"Processing octave %d with shape %s\" % (i, shape))\n",
    "    img = tf.image.resize(img, shape)\n",
    "    img = gradient_ascent_loop(\n",
    "        img, iterations=iterations, learning_rate=step, max_loss=max_loss\n",
    "    )\n",
    "    upscaled_shrunk_original_img = tf.image.resize(shrunk_original_img, shape)\n",
    "    same_size_original = tf.image.resize(original_img, shape)\n",
    "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
    "\n",
    "    img += lost_detail\n",
    "    shrunk_original_img = tf.image.resize(original_img, shape)\n",
    "\n",
    "tf.keras.preprocessing.image.save_img(result_prefix + \".png\", deprocess_image(img.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks as follows:\n",
    "\n",
    "<img src=./sils_maria_dream.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE \n",
    "Because the original Inception V3 network was trained to recognize\n",
    "concepts in images of size 299 × 299, and given that the process involves scaling\n",
    "the images down by a reasonable factor, the DeepDream implementation\n",
    "produces much better results on images that are somewhere between 300 ×\n",
    "300 and 400 × 400. Regardless, you can run the same code on images of any\n",
    "size and any ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We strongly suggest that you explore what you can do by adjusting which layers you\n",
    "use in your loss. Layers that are lower in the network contain more-local, less-abstract\n",
    "representations and lead to dream patterns that look more geometric. Layers that are\n",
    "higher up lead to more-recognizable visual patterns based on the most common\n",
    "objects found in ImageNet, such as dog eyes, bird feathers, and so on. You can use\n",
    "random generation of the parameters in the `layer_contributions` dictionary to\n",
    "quickly explore many different layer combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up\n",
    "\n",
    "- DeepDream consists of running a ConvNet in reverse to generate inputs based\n",
    "on the representations learned by the network.\n",
    "- The results produced are fun and somewhat similar to the visual artifacts\n",
    "induced in humans by the disruption of the visual cortex via psychedelics.\n",
    "- Note that the process isn’t specific to image models or even to convnets. It can\n",
    "be done for speech, music, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : Neural style transfer\n",
    "\n",
    "In addition to DeepDream, another major development in deep-learning-driven\n",
    "image modification is __neural style transfer__, introduced by Leon Gatys et al. in the summer\n",
    "of 2015. The neural style transfer algorithm has undergone many refinements\n",
    "and spawned many variations since its original introduction, and it has made its way\n",
    "into many smartphone photo apps. For simplicity, this section focuses on the formulation\n",
    "described in the original paper.\n",
    "\n",
    "\n",
    "Neural style transfer consists of applying the style of a reference image to a target\n",
    "image while conserving the content of the target image. The Figure below shows an example:\n",
    "    \n",
    "<img src='./Bilder/outlook.jpg'>\n",
    "\n",
    "\n",
    "\n",
    "In this context, _style_ essentially means textures, colors, and visual patterns in the image, at\n",
    "various spatial scales; and the _content_ is the higher-level macrostructure of the image.\n",
    "For instance, blue-and-yellow circular brushstrokes are considered to be the style in figure\n",
    "8.7 (using _Starry Night_ by Vincent Van Gogh), and the buildings in the Tübingen\n",
    "photograph are considered to be the content.\n",
    "\n",
    "The idea of style transfer, which is tightly related to that of texture generation, has\n",
    "had a long history in the image-processing community prior to the development of\n",
    "neural style transfer in 2015. But as it turns out, the deep-learning-based implementations\n",
    "of style transfer offer results unparalleled by what had been previously achieved\n",
    "with classical computer-vision techniques, and they triggered an amazing renaissance\n",
    "in creative applications of computer vision.\n",
    "\n",
    "The key notion behind implementing style transfer is the same idea that’s central\n",
    "to all deep-learning algorithms: you define a loss function to specify what you want to\n",
    "achieve, and you minimize this loss. You know what you want to achieve: conserving\n",
    "the content of the original image while adopting the style of the reference image. If\n",
    "we were able to mathematically define _content_ and _style_, then an appropriate loss function\n",
    "to minimize would be the following:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loss = distance(style(reference_image) - style(generated_image)) +\n",
    "distance(content(original_image) - content(generated_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, distance is a norm function such as the L2 norm, `content` is a function that\n",
    "takes an image and computes a representation of its content, and `style` is a function\n",
    "that takes an image and computes a representation of its style. Minimizing this\n",
    "loss causes style(generated_image) to be close to style(reference_image), and\n",
    "content(generated_image) is close to content(generated_image), thus achieving\n",
    "style transfer as we defined it.\n",
    "\n",
    "A fundamental observation made by Gatys et al. was that deep convolutional neural\n",
    "networks offer a way to mathematically define the style and content functions.\n",
    "Let’s see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The content loss\n",
    "\n",
    "As you already know, activations from earlier layers in a network contain _local_ information\n",
    "about the image, whereas activations from higher layers contain increasingly _global_,\n",
    "abstract information. Formulated in a different way, the activations of the different layers\n",
    "of a ConvNet provide a decomposition of the contents of an image over different spatial\n",
    "scales. Therefore, you’d expect the content of an image, which is more global and\n",
    "_abstract_, to be captured by the representations of the upper layers in a ConvNet.\n",
    "\n",
    "\n",
    "A good candidate for content loss is thus the L2 norm between the activations of\n",
    "an upper layer in a pretrained convnet, computed over the target image, and the activations\n",
    "of the same layer computed over the generated image. This guarantees that, as\n",
    "seen from the upper layer, the generated image will look similar to the original target\n",
    "image. Assuming that what the upper layers of a ConvNet see is really the content of\n",
    "their input images, then this works as a way to preserve image content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The style loss\n",
    "\n",
    "The content loss only uses a single upper layer, but the style loss as defined by Gatys\n",
    "et al. uses multiple layers of a ConvNet: you try to capture the appearance of the stylereference\n",
    "image at all spatial scales extracted by the ConvNet, not just a single scale.\n",
    "\n",
    "For the _style loss_, Gatys et al. use the _Gram matrix_ of a layer’s activations: the inner\n",
    "product of the feature maps of a given layer. This inner product can be understood as\n",
    "representing a map of the correlations between the layer’s features. These feature correlations\n",
    "capture the statistics of the patterns of a particular spatial scale, which empirically\n",
    "correspond to the appearance of the textures found at this scale.\n",
    "Hence, the style loss aims to preserve similar internal correlations within the activations\n",
    "of different layers, across the style-reference image and the generated image. In\n",
    "turn, this guarantees that the textures found at different spatial scales look similar\n",
    "across the style-reference image and the generated image.\n",
    "In short, you can use a pretrained convnet to define a loss that will do the following:\n",
    "\n",
    "- Preserve content by maintaining similar high-level layer activations between the\n",
    "target content image and the generated image. The ConvNet should “see” both\n",
    "the target image and the generated image as containing the same things.\n",
    "\n",
    "- Preserve style by maintaining similar correlations within activations for both lowlevel\n",
    "layers and high-level layers. Feature correlations capture textures : the generated\n",
    "image and the style-reference image should share the same textures at\n",
    "different spatial scales.\n",
    "\n",
    "Now, let’s look at a Keras implementation of the original 2015 neural style transfer\n",
    "algorithm. As you’ll see, it shares many similarities with the DeepDream implementation\n",
    "developed in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural style transfer in Keras\n",
    "\n",
    "Neural style transfer can be implemented using any pretrained ConvNet. Here, you’ll\n",
    "use the VGG19 network used by Gatys et al. VGG19 is a simple variant of the VGG16 network\n",
    "introduced in chapter 5, with three more convolutional layers.\n",
    "This is the general process:\n",
    "\n",
    "1. Set up a network that computes VGG19 layer activations for the style-reference\n",
    "image, the target image, and the generated image at the same time.\n",
    "\n",
    "2. Use the layer activations computed over these three images to define the loss\n",
    "function described earlier, which you’ll minimize in order to achieve style\n",
    "transfer.\n",
    "\n",
    "3. Set up a gradient-descent process to minimize this loss function.\n",
    "Let’s start by defining the paths to the style-reference image and the target image. To\n",
    "make sure that the processed images are a similar size (widely different sizes make\n",
    "style transfer more difficult), you’ll later resize them all to a shared height of 400 px.\n",
    "\n",
    "Let’s start by defining the paths to the style-reference image and the target image. To\n",
    "make sure that the processed images are a similar size (widely different sizes make\n",
    "style transfer more difficult), you’ll later resize them all to a shared height of 400 px."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import vgg19\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "# Path to the image you want to transform\n",
    "base_image_path = './Bilder/sils.jpg'\n",
    "\n",
    "# Path to the style image\n",
    "style_reference_image_path = './Bilder/transfer_style_reference.jpg'\n",
    "\n",
    "# filename of result\n",
    "result_prefix = \"sils_maria_generated\"\n",
    "\n",
    "# Weights of the different loss components\n",
    "total_variation_weight = 1e-6\n",
    "style_weight = 1e-6\n",
    "content_weight = 2.5e-8\n",
    "\n",
    "\n",
    "# Dimensions of the generated picture.\n",
    "width, height = keras.preprocessing.image.load_img(base_image_path).size\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our base (content) image and our style reference image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(base_image_path))\n",
    "display(Image(style_reference_image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need some auxiliary functions for loading, preprocessing, and postprocessing the\n",
    "images that go in and out of the VGG19 convnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    # Util function to open, resize and format pictures into appropriate tensors\n",
    "    img = keras.preprocessing.image.load_img(\n",
    "        image_path, target_size=(img_nrows, img_ncols)\n",
    "    )\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return tf.convert_to_tensor(img)\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Util function to convert a tensor into a valid image\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s set up the VGG19 network. It takes as input a batch of three images: the stylereference\n",
    "image, the target image, and a placeholder that will contain the generated\n",
    "image. A placeholder is a symbolic tensor, the values of which are provided externally\n",
    "via Numpy arrays. The style-reference and target image are static and thus defined\n",
    "using K.constant, whereas the values contained in the placeholder of the generated\n",
    "image will change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the pretrained VGG19 network and applying it to the three images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a VGG19 model loaded with pre-trained ImageNet weights\n",
    "model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "# Set up a model that returns the activation values for every layer in\n",
    "# VGG19 (as a dict).\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define the content loss, which will make sure the top layer of the VGG19 ConvNet\n",
    "has a similar view of the target image and the generated image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An auxiliary loss function\n",
    "# designed to maintain the \"content\" of the\n",
    "# base image in the generated image\n",
    "\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    return tf.reduce_sum(tf.square(combination - base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the style loss. It uses an auxiliary function to compute the Gram matrix of an\n",
    "input matrix: a map of the correlations found in the original feature matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gram matrix of an image tensor (feature-wise outer product)\n",
    "\n",
    "\n",
    "def gram_matrix(x):\n",
    "    x = tf.transpose(x, (2, 0, 1))\n",
    "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "    gram = tf.matmul(features, tf.transpose(features))\n",
    "    return gram\n",
    "\n",
    "# The \"style loss\" is designed to maintain\n",
    "# the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of\n",
    "# feature maps from the style reference image\n",
    "# and from the generated image\n",
    "\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To these two loss components, you add a third: the total variation loss, which operates\n",
    "on the pixels of the generated combination image. It encourages spatial continuity in\n",
    "the generated image, thus avoiding overly pixelated results. You can interpret it as a\n",
    "regularization loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total variation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3rd loss function, total variation loss,\n",
    "# designed to keep the generated image locally coherent\n",
    "\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    a = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :]\n",
    "    )\n",
    "    b = tf.square(\n",
    "        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :]\n",
    "    )\n",
    "    return tf.reduce_sum(tf.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss that you minimize is a weighted average of these three losses. To compute the\n",
    "content loss, you use only one upper layer—the `block5_conv2` layer—whereas for the\n",
    "style loss, you use a list of layers that spans both low-level and high-level layers. You\n",
    "add the total variation loss at the end.\n",
    "Depending on the style-reference image and content image you’re using, you’ll\n",
    "likely want to tune the `content_weight` coefficient (the contribution of the content\n",
    "loss to the total loss). A higher content_weight means the target content will be more\n",
    "recognizable in the generated image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the final loss that you’ll minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of layers to use for the style loss.\n",
    "style_layer_names = [\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "]\n",
    "# The layer to use for the content loss.\n",
    "content_layer_name = \"block5_conv2\"\n",
    "\n",
    "\n",
    "def compute_loss(combination_image, base_image, style_reference_image):\n",
    "    input_tensor = tf.concat(\n",
    "        [base_image, style_reference_image, combination_image], axis=0\n",
    "    )\n",
    "    features = feature_extractor(input_tensor)\n",
    "\n",
    "    # Initialize the loss\n",
    "    loss = tf.zeros(shape=())\n",
    "\n",
    "    # Add content loss\n",
    "    layer_features = features[content_layer_name]\n",
    "    base_image_features = layer_features[0, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    loss = loss + content_weight * content_loss(\n",
    "        base_image_features, combination_features\n",
    "    )\n",
    "    # Add style loss\n",
    "    for layer_name in style_layer_names:\n",
    "        layer_features = features[layer_name]\n",
    "        style_reference_features = layer_features[1, :, :, :]\n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "        sl = style_loss(style_reference_features, combination_features)\n",
    "        loss += (style_weight / len(style_layer_names)) * sl\n",
    "\n",
    "    # Add total variation loss\n",
    "    loss += total_variation_weight * total_variation_loss(combination_image)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you’ll set up the gradient-descent process. In the original Gatys et al. paper,\n",
    "optimization is performed using the L-BFGS algorithm, so that’s what you’ll use here.\n",
    "This is a key difference from the DeepDream example in section 8.2. The L-BFGS algorithm\n",
    "comes packaged with SciPy, but there are two slight limitations with the SciPy\n",
    "implementation:\n",
    "- It requires that you pass the value of the loss function and the value of the gradients\n",
    "as two separate functions.\n",
    "- It can only be applied to flat vectors, whereas you have a 3D image array.\n",
    "It would be inefficient to compute the value of the loss function and the value of the\n",
    "gradients independently, because doing so would lead to a lot of redundant computation\n",
    "between the two; the process would be almost twice as slow as computing them\n",
    "jointly. To bypass this, you’ll set up a Python class named Evaluator that computes\n",
    "both the loss value and the gradients value at once, returns the loss value when called\n",
    "the first time, and caches the gradients for the next call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the gradient-descent process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(combination_image, base_image, style_reference_image)\n",
    "    grads = tape.gradient(loss, combination_image)\n",
    "    return loss, grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can run the gradient-ascent process using SciPy’s L-BFGS algorithm, saving\n",
    "the current generated image at each iteration of the algorithm (here, a single iteration\n",
    "represents 20 steps of gradient ascent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Style-transfer loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(\n",
    "    keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n",
    "    )\n",
    ")\n",
    "\n",
    "base_image = preprocess_image(base_image_path)\n",
    "style_reference_image = preprocess_image(style_reference_image_path)\n",
    "combination_image = tf.Variable(preprocess_image(base_image_path))\n",
    "\n",
    "iterations = 4000\n",
    "for i in range(1, iterations + 1):\n",
    "    loss, grads = compute_loss_and_grads(\n",
    "        combination_image, base_image, style_reference_image\n",
    "    )\n",
    "    optimizer.apply_gradients([(grads, combination_image)])\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n",
    "        img = deprocess_image(combination_image.numpy())\n",
    "        fname = result_prefix + \"_at_iteration_%d.png\" % i\n",
    "        keras.preprocessing.image.save_img(fname, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Figure below shows what you get: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(result_prefix + \"_at_iteration_2300.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that what this technique achieves is\n",
    "merely a form of image retexturing, or texture transfer. It works best with stylereference\n",
    "images that are strongly textured and highly self-similar, and with content\n",
    "targets that don’t require high levels of detail in order to be recognizable. It typically\n",
    "can’t achieve fairly abstract feats such as transferring the style of one portrait to\n",
    "another. The algorithm is closer to classical signal processing than to AI, so don’t\n",
    "expect it to work like magic!\n",
    "\n",
    "\n",
    "Additionally, note that running this style-transfer algorithm is slow. But the transformation\n",
    "operated by the setup is simple enough that it can be learned by a small, fast\n",
    "feedforward convnet as well—as long as you have appropriate training data available.\n",
    "Fast style transfer can thus be achieved by first spending a lot of compute cycles to\n",
    "generate input-output training examples for a fixed style-reference image, using the\n",
    "method outlined here, and then training a simple convnet to learn this style-specific\n",
    "transformation. Once that’s done, stylizing a given image is instantaneous: it’s just a\n",
    "forward pass of this small convnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping up\n",
    "- Style transfer consists of creating a new image that preserves the contents of a\n",
    "target image while also capturing the style of a reference image.\n",
    "- Content can be captured by the high-level activations of a convnet.\n",
    "- Style can be captured by the internal correlations of the activations of different\n",
    "layers of a convnet.\n",
    "- Hence, deep learning allows style transfer to be formulated as an optimization\n",
    "process using a loss defined with a pretrained convnet.\n",
    "- Starting from this basic idea, many variants and refinements are possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "1. Resize style image before running style transfer.\n",
    "\n",
    "2. Mix style from multiple images.\n",
    "\n",
    "3. Give more weight on content image or style image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III : Generating Images with Variational Autoencoders (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling from a latent space of images to create entirely new images or edit existing\n",
    "ones is currently the most popular and successful application of creative AI. In this section\n",
    "and the next, we’ll review some high-level concepts pertaining to image generation,\n",
    "alongside implementations details relative to the two main techniques in this\n",
    "domain: _variational autoencoders_ (VAEs) and _generative adversarial networks_ (GANs). \n",
    "\n",
    "The techniques we present here aren’t specific to images—you could develop latent spaces\n",
    "of sound, music, or even text, using GANs and VAEs—but in practice, the most interesting\n",
    "results have been obtained with pictures, and that’s what we focus on here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from Latent Spaces of Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea of image generation is to develop a low-dimensional latent space of representations\n",
    "(which naturally is a vector space) where any point can be mapped to a\n",
    "realistic-looking image. The module capable of realizing this mapping, taking as input\n",
    "a latent point and outputting an image (a grid of pixels), is called a _generator_ (in the\n",
    "case of GANs) or a _decoder_ (in the case of VAEs). \n",
    "\n",
    "Once such a latent space has been developed, you can sample points from it, either deliberately \n",
    "or at random, and, by mapping them to image space, generate images that have never been seen before.\n",
    "\n",
    "<img src='./Bilder/latent_space.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs and VAEs are two different strategies for learning such latent spaces of image\n",
    "representations, each with its own characteristics. VAEs are great for learning latent\n",
    "spaces that are well structured, where specific directions encode a meaningful axis of\n",
    "variation in the data. GANs generate images that can potentially be highly realistic, but\n",
    "the latent space they come from may not have as much structure and continuity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept vectors for image editing\n",
    "\n",
    "The idea of concept vectors is the following : given a latent space of representations, or an\n",
    "embedding space, certain directions in the space may encode interesting axes of variation\n",
    "in the original data. \n",
    "\n",
    "In a latent space of images of faces, for instance, there may\n",
    "be a smile vector $s$, such that if latent point $z$ is the embedded representation of a certain\n",
    "face, then latent point $z + s$ is the embedded representation of the same face,\n",
    "smiling. \n",
    "\n",
    "\n",
    "Once you’ve identified such a vector, it then becomes possible to edit images\n",
    "by projecting them into the latent space, moving their representation in a meaningful\n",
    "way, and then decoding them back to image space. There are concept vectors for\n",
    "essentially any independent dimension of variation in image space—in the case of\n",
    "faces, you may discover vectors for adding sunglasses to a face, removing glasses, turning\n",
    "a male face into a female face, and so on. \n",
    "\n",
    "\n",
    "The Figure below is an example of a smile vector,\n",
    "a concept vector discovered by Tom White from the Victoria University School of\n",
    "Design in New Zealand, using VAEs trained on a dataset of faces of celebrities (the\n",
    "CelebA dataset).\n",
    "\n",
    "<img src='./Bilder/smile_vector.jpg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational autoencoders\n",
    "\n",
    "Variational autoencoders, simultaneously discovered by Kingma and Welling in\n",
    "December 2013 and Rezende, Mohamed, and Wierstra in January 2014 are a kind\n",
    "of generative model that’s especially appropriate for the task of image editing via concept\n",
    "vectors. They’re a modern take on autoencoders — a type of network that aims to\n",
    "encode an input to a low-dimensional latent space and then decode it back—that\n",
    "mixes ideas from deep learning with Bayesian inference.\n",
    "\n",
    "\n",
    "A classical image autoencoder takes an image, maps it to a latent vector space via\n",
    "an encoder module, and then decodes it back to an output with the same dimensions\n",
    "as the original image, via a decoder module,  see the figure below: \n",
    "\n",
    "<img src='./Bilder/autoencoder.jpg'>\n",
    "\n",
    "It’s then trained by\n",
    "using as target data the same images as the input images, meaning the autoencoder\n",
    "learns to reconstruct the original inputs. By imposing various constraints on the code\n",
    "(the output of the encoder), you can get the autoencoder to learn more-or-less interesting\n",
    "latent representations of the data. \n",
    "\n",
    "Most commonly, you’ll constrain the code to\n",
    "be low-dimensional and sparse (mostly zeros), in which case the encoder acts as a way\n",
    "to compress the input data into fewer bits of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, such classical autoencoders don’t lead to particularly useful or nicely\n",
    "structured latent spaces. They’re not much good at compression, either. For these reasons,\n",
    "they have largely fallen out of fashion. VAEs, however, augment autoencoders\n",
    "with a little bit of statistical magic that forces them to learn continuous, highly structured\n",
    "latent spaces. They have turned out to be a powerful tool for image generation.\n",
    "\n",
    "\n",
    "A VAE, instead of compressing its input image into a fixed code in the latent space,\n",
    "turns the image into the parameters of a statistical distribution: a mean and a variance.\n",
    "Essentially, this means you’re assuming the input image has been generated by a\n",
    "statistical process, and that the randomness of this process should be taken into\n",
    "account during encoding and decoding. The VAE then uses the mean and variance\n",
    "parameters to randomly sample one element of the distribution, and decodes that element\n",
    "back to the original input, see the Figure below: \n",
    "\n",
    "\n",
    "<img src='./Bilder/vae_illustration.jpg'>\n",
    "\n",
    "A VAE maps an image to two vectors, `z_mean` and `z_log_sigma`, which define\n",
    "a probability distribution over the latent space, used to sample a latent point to decode.\n",
    "\n",
    "\n",
    "The stochasticity of this process\n",
    "improves robustness and forces the latent space to encode meaningful representations\n",
    "everywhere: every point sampled in the latent space is decoded to a valid output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In technical terms, here’s how a VAE works:\n",
    "\n",
    "1. An encoder module turns the input samples input_img into two parameters in\n",
    "a latent space of representations, `z_mean` and `z_log_variance`\n",
    "\n",
    "2. You randomly sample a point z from the latent normal distribution that’s\n",
    "assumed to generate the input image, via\n",
    "`z = z_mean + exp(z_log_variance) * epsilon`\n",
    "\n",
    "where epsilon is a random tensor of small values.\n",
    "\n",
    "3. A decoder module maps this point in the latent space back to the original input\n",
    "image.\n",
    "\n",
    "\n",
    "\n",
    "Because `epsilon` is random, the process ensures that every point that’s close to the latent location\n",
    "where you encoded `input_img` (z-mean) can be decoded to something similar to\n",
    "`input_img`, thus forcing the latent space to be continuously meaningful. Any two close points\n",
    "in the latent space will decode to highly similar images. Continuity, combined with the low\n",
    "dimensionality of the latent space, forces every direction in the latent space to encode a meaningful\n",
    "axis of variation of the data, making the latent space very structured and thus highly suitable\n",
    "to manipulation via concept vectors.\n",
    "\n",
    "\n",
    "\n",
    "The parameters of a VAE are trained via two loss functions: a _reconstruction loss_ that\n",
    "forces the decoded samples to match the initial inputs, and a _regularization loss_ that\n",
    "helps learn well-formed latent spaces and reduce overfitting to the training data. Let’s\n",
    "quickly go over a Keras implementation of a VAE. Schematically, it looks like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Encodes the input into a mean and variance parameter\n",
    "z_mean, z_log_variance = encoder(input_img)\n",
    "\n",
    "# Draws a latent point using a small random epsilon\n",
    "z = z_mean + exp(z_log_variance) * epsilon\n",
    "\n",
    "# Decodes z back to an image\n",
    "reconstructed_img = decoder(z)\n",
    "\n",
    "# Instantiates the autoencoder model, which maps an \n",
    "# input image to its reconstruction\n",
    "model = Model(input_img, reconstructed_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then train the model using the reconstruction loss and the regularization loss.\n",
    "The following listing shows the encoder network you’ll use, mapping images to the\n",
    "parameters of a probability distribution over the latent space. It’s a simple convnet\n",
    "that maps the input image x to two vectors, `z_mean` and `z_log_var`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent-space-sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE encoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the code for using `z_mean` and `z_log_var`, the parameters of the statistical distribution\n",
    "assumed to have produced `input_img`, to generate a latent space point z.\n",
    "Here, you wrap some arbitrary code (built on top of Keras backend primitives) into a\n",
    "`Lambda` layer. In Keras, everything needs to be a layer, so code that isn’t part of a builtin\n",
    "layer should be wrapped in a `Lambda` (or in a custom layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE decoder network, mapping latent space points to images\n",
    "\n",
    "The following listing shows the decoder implementation. You reshape the vector z to\n",
    "the dimensions of an image and then use a few convolution layers to obtain a final\n",
    "image output that has the same dimensions as the original `input_img`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual loss of a VAE doesn’t fit the traditional expectation of a sample-wise function\n",
    "of the form `loss(input, target)`. Thus, you’ll set up the loss by writing a custom\n",
    "layer that internally uses the built-in `add_loss` layer method to create an arbitrary loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom layer used to compute the VAE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = encoder(data)\n",
    "            reconstruction = decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                keras.losses.binary_crossentropy(data, reconstruction)\n",
    "            )\n",
    "            reconstruction_loss *= 28 * 28\n",
    "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you’re ready to instantiate and train the model. Because the loss is taken care\n",
    "of in the custom layer, you don’t specify an external loss at compile time (`loss=None`),\n",
    "which in turn means you won’t pass target data during training (as you can see, you\n",
    "only pass `x_train` to the model in `fit`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
    "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(mnist_digits, epochs=30, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once such a model is trained—on MNIST, in this case—you can use the decoder network\n",
    "to turn arbitrary latent space vectors into images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling a grid of points from the 2D latent space and decoding them to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_latent(encoder, decoder):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    scale = 2.0\n",
    "    figsize = 15\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid of sampled digits (see the Figure above) shows a completely continuous\n",
    "distribution of the different digit classes, with one digit morphing\n",
    "into another as you follow a path through latent space. Specific directions\n",
    "in this space have a meaning: for example, there’s a direction for\n",
    "“four-ness,” “one-ness,” and so on.\n",
    "\n",
    "\n",
    "In the next section, we’ll cover in detail the other major tool for generating\n",
    "artificial images: generative adversarial networks (GANs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up \n",
    "\n",
    "Wrapping up\n",
    "- Image generation with deep learning is done by learning latent spaces that capture\n",
    "statistical information about a dataset of images. By sampling and decoding\n",
    "points from the latent space, you can generate never-before-seen images.\n",
    "There are two major tools to do this: VAEs and GANs.\n",
    "\n",
    "- VAEs result in highly structured, continuous latent representations. For this reason,\n",
    "they work well for doing all sorts of image editing in latent space: face\n",
    "swapping, turning a frowning face into a smiling face, and so on. They also work\n",
    "nicely for doing latent-space-based animations, such as animating a walk along a\n",
    "cross section of the latent space, showing a starting image slowly morphing into\n",
    "different images in a continuous way.\n",
    "\n",
    "- GANs enable the generation of realistic single-frame images but may not induce\n",
    "latent spaces with solid structure and high continuity.\n",
    "Most successful practical applications I have seen with images rely on VAEs, but GANs\n",
    "are extremely popular in the world of academic research—at least, circa 2016–2017.\n",
    "You’ll find out how they work and how to implement one in the next section.\n",
    "\n",
    "## Extensions for VAE\n",
    "\n",
    "\n",
    "To play further with image generation, I suggest working with the [Largescale\n",
    "Celeb Faces Attributes](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) (CelebA) dataset. It’s a free-to-download image\n",
    "dataset containing more than 200,000 celebrity portraits. It’s great for experimenting\n",
    "with concept vectors in particular—it definitely beats MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV : Adverserial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative adversarial networks (GANs), introduced in 2014 by \n",
    "[Goodfellow et al.,](https://arxiv.org/abs/1406.2661) are\n",
    "an alternative to VAEs for learning latent spaces of images. They enable the generation\n",
    "of fairly realistic synthetic images by forcing the generated images to be statistically\n",
    "almost indistinguishable from real ones.\n",
    "\n",
    "\n",
    "An intuitive way to understand GANs is to imagine a forger trying to create a fake\n",
    "Picasso painting. At first, the forger is pretty bad at the task. He mixes some of his\n",
    "fakes with authentic Picassos and shows them all to an art dealer. The art dealer makes\n",
    "an authenticity assessment for each painting and gives the forger feedback about what\n",
    "makes a Picasso look like a Picasso. The forger goes back to his studio to prepare some\n",
    "new fakes. As times goes on, the forger becomes increasingly competent at imitating\n",
    "the style of Picasso, and the art dealer becomes increasingly expert at spotting fakes.\n",
    "In the end, they have on their hands some excellent fake Picassos.\n",
    "\n",
    "\n",
    "That’s what a GAN is: a forger network and an expert network, each being trained\n",
    "to best the other. As such, a GAN is made of two parts:\n",
    "\n",
    "- _Generator network_ — Takes as input a random vector (a random point in the\n",
    "latent space), and decodes it into a synthetic image\n",
    "- _Discriminator network_ (or adversary) — Takes as input an image (real or synthetic),\n",
    "and predicts whether the image came from the training set or was created by\n",
    "the generator network.\n",
    "\n",
    "The generator network is trained to be able to fool the discriminator network, and\n",
    "thus it evolves toward generating increasingly realistic images as training goes on: artificial\n",
    "images that look indistinguishable from real ones, to the extent that it’s impossible\n",
    "for the discriminator network to tell the two apart (see figure 8.15). Meanwhile,\n",
    "the discriminator is constantly adapting to the gradually improving capabilities of the\n",
    "generator, setting a high bar of realism for the generated images. Once training is\n",
    "over, the generator is capable of turning any point in its input space into a believable\n",
    "image. Unlike VAEs, this latent space has fewer explicit guarantees of meaningful\n",
    "structure; in particular, it isn’t continuous.\n",
    "\n",
    "\n",
    "<img src='./Bilder/gan_illustration.jpg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarkably, a GAN is a system where the optimization minimum isn’t fixed, unlike in\n",
    "any other training setup you’ve encountered in this book. Normally, gradient descent\n",
    "consists of rolling down hills in a static loss landscape. But with a GAN, every step\n",
    "taken down the hill changes the entire landscape a little. \n",
    "\n",
    "It’s a dynamic system where\n",
    "the optimization process is seeking not a minimum, but an equilibrium between two\n",
    "forces. For this reason, GANs are notoriously difficult to train—getting a GAN to work\n",
    "requires lots of careful tuning of the model architecture and training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A schematic GAN implementation\n",
    "\n",
    "In this section, we’ll explain how to implement a GAN in Keras, in its barest form—\n",
    "because GANs are advanced, diving deeply into the technical details would be out of\n",
    "scope for this chapter. The specific implementation is a _deep convolutional GAN_ (DCGAN):\n",
    "a GAN where the generator and discriminator are deep convnets. In particular, it uses\n",
    "a `Conv2DTranspose` layer for image upsampling in the generator.\n",
    "You’ll train the GAN on images from CIFAR10, a dataset of 50,000 32 × 32 RGB\n",
    "images belonging to 10 classes (5,000 images per class). To make things easier, you’ll\n",
    "only use images belonging to the class “frog.”\n",
    "Schematically, the GAN looks like this:\n",
    "\n",
    "1. A generator network maps vectors of shape (`latent_dim`) to images of shape\n",
    "$(32, 32, 3)$.\n",
    "\n",
    "2. A discriminator network maps images of shape $(32, 32, 3)$ to a binary score\n",
    "estimating the probability that the image is real.\n",
    "\n",
    "3. A gan network chains the generator and the discriminator together: \n",
    "`gan(x) = discriminator(generator(x))`. Thus this gan network maps latent space vectors\n",
    "to the discriminator’s assessment of the realism of these latent vectors as\n",
    "decoded by the generator.\n",
    "\n",
    "\n",
    "4. You train the discriminator using examples of real and fake images along with\n",
    "“real”/“fake” labels, just as you train any regular image-classification model.\n",
    "\n",
    "5. To train the generator, you use the gradients of the generator’s weights with\n",
    "regard to the loss of the gan model. This means, at every step, you move the\n",
    "weights of the generator in a direction that makes the discriminator more likely\n",
    "to classify as “real” the images decoded by the generator. In other words, you\n",
    "train the generator to fool the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bag of tricks\n",
    "\n",
    "The process of training GANs and tuning GAN implementations is notoriously difficult.\n",
    "There are a number of known tricks you should keep in mind. Like most things\n",
    "in deep learning, it’s more alchemy than science: these tricks are heuristics, not\n",
    "theory-backed guidelines. They’re supported by a level of intuitive understanding of\n",
    "the phenomenon at hand, and they’re known to work well empirically, although not\n",
    "necessarily in every context.\n",
    "Here are a few of the tricks used in the implementation of the GAN generator and\n",
    "discriminator in this section. It isn’t an exhaustive list of GAN-related tips; you’ll find\n",
    "many more across the GAN literature:\n",
    "\n",
    "- We use `tanh` as the last activation in the generator, instead of `sigmoid`, which is\n",
    "more commonly found in other types of models.\n",
    "\n",
    "- We sample points from the latent space using a _normal distribution_ (Gaussian distribution),\n",
    "not a uniform distribution.\n",
    "\n",
    "- Stochasticity is good to induce robustness. Because GAN training results in a\n",
    "dynamic equilibrium, GANs are likely to get stuck in all sorts of ways. Introducing\n",
    "randomness during training helps prevent this. We introduce randomness\n",
    "in two ways: by using dropout in the discriminator and by adding random noise\n",
    "to the labels for the discriminator.\n",
    "\n",
    "- Sparse gradients can hinder GAN training. In deep learning, sparsity is often a\n",
    "desirable property, but not in GANs. Two things can induce gradient sparsity:\n",
    "max pooling operations and `ReLU` activations. Instead of max pooling, we recommend\n",
    "using strided convolutions for downsampling, and we recommend\n",
    "using a `LeakyReLU` layer instead of a ReLU activation. It’s similar to `ReLU`, but it\n",
    "relaxes sparsity constraints by allowing small negative activation values.\n",
    "\n",
    "- In generated images, it’s common to see checkerboard artifacts caused by\n",
    "unequal coverage of the pixel space in the generator (see figure 8.17). To fix\n",
    "this, we use a kernel size that’s divisible by the stride size whenever we use a\n",
    "strided `Conv2DTranpose` or `Conv2D` in both the generator and the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generator\n",
    "\n",
    "First, let’s develop a `generator` model that turns a vector (from the latent space—\n",
    "during training it will be sampled at random) into a candidate image. One of the\n",
    "many issues that commonly arise with GANs is that the generator gets stuck with generated\n",
    "images that look like noise. A possible solution is to use dropout on both the discriminator\n",
    "and the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "\n",
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "# Transforms the input into a 16 × 16 128-channel feature map\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16, 16, 128))(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "# Upsamples to 32 × 32\n",
    "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Produces a 32 × 32 1-channel feature map (shape of a CIFAR10 image)\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "# Instantiates the generator model, which maps the input\n",
    "# of shape (latent_dim,) into an image of shape (32, 32, 3)\n",
    "generator = keras.models.Model(generator_input, x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The discriminator\n",
    "\n",
    "Next, you’ll develop a discriminator model that takes as input a candidate image\n",
    "(real or synthetic) and classifies it into one of two classes: “generated image” or “real\n",
    "image that comes from the training set.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The GAN discriminator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "# One dropout layer: an important trick!\n",
    "x = layers.Dropout(0.4)(x)\n",
    "# Classification layer\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "# Instantiates the discriminator model, which turns\n",
    "# a (32, 32, 3) input into a binary classification\n",
    "# decision (fake/real)\n",
    "discriminator = tensorflow.keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()\n",
    "discriminator_optimizer = tensorflow.keras.optimizers.RMSprop(\n",
    "lr=0.0008,\n",
    "    # Uses gradient clipping (by value) in the optimizer\n",
    "clipvalue=1.0,\n",
    "    # To stabilize training, uses learning-rate decay\n",
    "decay=1e-8)\n",
    "discriminator.compile(optimizer=discriminator_optimizer,\n",
    "loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The adversarial network\n",
    "\n",
    "Finally, you’ll set up the GAN, which chains the generator and the discriminator.\n",
    "When trained, this model will move the generator in a direction that improves its ability\n",
    "to fool the discriminator. This model turns latent-space points into a classification\n",
    "decision—“fake” or “real”—and it’s meant to be trained with labels that are always\n",
    "“these are real images.” So, training gan will update the weights of `generator` in a way\n",
    "that makes discriminator more likely to predict “real” when looking at fake images.\n",
    "It’s very important to note that you set the `discriminator` to be frozen during training\n",
    "(non-trainable): its weights won’t be updated when training gan. If the discriminator\n",
    "weights could be updated during this process, then you’d be training the discriminator\n",
    "to always predict “real,” which isn’t what you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "gan_input = tensorflow.keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = tensorflow.keras.models.Model(gan_input, gan_output)\n",
    "gan_optimizer = tensorflow.keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train your DCGAN\n",
    "\n",
    "Now you can begin training. To recapitulate, this is what the training loop looks like\n",
    "schematically. For each epoch, you do the following:\n",
    "1. Draw random points in the latent space (random noise).\n",
    "\n",
    "2. Generate images with `generator` using this random noise\n",
    "\n",
    "3. Mix the generated images with real ones\n",
    "\n",
    "4. Train `discriminator` using these mixed images, with corresponding targets:\n",
    "either “real” (for the real images) or “fake” (for the generated images)\n",
    "\n",
    "5. Draw new random points in the latent space\n",
    "\n",
    "6. Train gan using these random vectors, with targets that all say “these are real\n",
    "images.” This updates the weights of the generator (only, because the discriminator\n",
    "is frozen inside gan) to move them toward getting the discriminator to\n",
    "predict “these are real images” for generated images: this trains the generator\n",
    "to fool the discriminator.\n",
    "Let’s implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "# Loads CIFAR10 data\n",
    "(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Selects frog images (class 6)\n",
    "x_train = x_train[y_train.flatten() == 6]\n",
    "\n",
    "# Normalizes data\n",
    "x_train = x_train.reshape((x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\n",
    "iterations = 10000\n",
    "batch_size = 20\n",
    "\n",
    "# Specifies where you want to save generated images\n",
    "save_dir = './data/'\n",
    "start = 0\n",
    "for step in range(iterations):\n",
    "    # Samples random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    # Decodes them to fake images\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "    # Combines them with real images\n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start: stop]\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "    # Assembles labels, discriminating real from fake images\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "    # Adds random noise to the labels—an important trick!\n",
    "    labels += 0.05 * np.random.random(labels.shape)\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "    # Samples random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    # Assembles labels that say “these are all real images” (it’s a lie!)\n",
    "    misleading_targets = np.zeros((batch_size, 1))\n",
    "    # Trains the generator (via the gan model, where the discriminator weights are frozen)\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "    \n",
    "    # Occasionally saves and plots (every 100 steps)\n",
    "    if step % 100 == 0:\n",
    "        # Saves model weights\n",
    "        gan.save_weights('gan.h5')\n",
    "        # Prints metrics\n",
    "        print('discriminator loss:', d_loss)\n",
    "        print('adversarial loss:', a_loss)\n",
    "        # Saves one generated image\n",
    "        img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'generated_frog' + str(step) + '.png'))\n",
    "        # Saves one real image for comparison\n",
    "        img = image.array_to_img(real_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'real_frog' + str(step) + '.png'))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, you may see the adversarial loss begin to increase considerably, while\n",
    "the discriminative loss tends to zero—the discriminator may end up dominating the\n",
    "generator. If that’s the case, try reducing the discriminator learning rate, and increase\n",
    "the dropout rate of the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "- A GAN consists of a generator network coupled with a discriminator network.\n",
    "The discriminator is trained to differenciate between the output of the generator\n",
    "and real images from a training dataset, and the generator is trained to fool the\n",
    "discriminator. Remarkably, the generator never sees images from the training\n",
    "set directly; the information it has about the data comes from the discriminator.\n",
    "\n",
    "- GANs are difficult to train, because training a GAN is a dynamic process rather\n",
    "than a simple gradient descent process with a fixed loss landscape. Getting a\n",
    "GAN to train correctly requires using a number of heuristic tricks, as well as\n",
    "extensive tuning.\n",
    "\n",
    "- GANs can potentially produce highly realistic images. But unlike VAEs, the\n",
    "latent space they learn doesn’t have a neat continuous structure and thus may\n",
    "not be suited for certain practical applications, such as image editing via latentspace\n",
    "concept vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
